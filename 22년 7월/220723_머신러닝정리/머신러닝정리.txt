# 기본적으로 훈련/테스트 셋으로 데이터를 나눠줌

from sklearn.model_selection import train_test_split
data = train[['Sex', 'Pclass', 'Age', 'Parch', 'Embarked']].to_numpy()
target = train['Survived'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2)

# 스케일링을 통해서 표준화로 옮겨줌

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_validate
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# 로지스틱회귀

**from sklearn.linear_model import LogisticRegression**
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
scores = cross_validate(lr, train_input, train_target, return_train_score=True, n_jobs=-1)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

# 선형회귀

**from sklearn.linear_model import LinearRegression**
lr1 = LinearRegression()
lr1.fit(train_scaled, train_target)
scores = cross_validate(lr1, train_input, train_target, return_train_score=True, n_jobs=-1)
print(lr1.score(train_scaled, train_target))
print(lr1.score(test_scaled, test_target))
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# SGDC 회귀

**from sklearn.linear_model import SGDClassifier**
sc = SGDClassifier(loss ='log', max_iter=10, random_state=0)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# GridSearchCV

**from sklearn.model_selection import GridSearchCV**
params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state=0), params, n_jobs=-1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_
print(dt.score(train_input, train_target))

# 랜덤포레스트

**from sklearn.ensemble import RandomForestClassifier**
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 결정트리

**from sklearn.tree import DecisionTreeClassifier**
dt = DecisionTreeClassifier()
dt.fit(train_scaled, train_target)
scores = cross_validate(dt, train_input, train_target, return_train_score=True, n_jobs=-1)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

# 플롯트리

**from sklearn.tree import plot_tree**
plt.figure(figsize=(2,4))
plot_tree(dt)
plt.show()

plt.figure(figsize=(10,5))
plot_tree(dt, max_depth=3, filled=True, feature_names=['Sex', 'Pclass', 'Age', 'Parch', 'Embarked'])
plt.show()

# 최근접 이웃

**from sklearn.neighbors import KNeighborsClassifier**
kn = KNeighborsClassifier()
kn.fit(train_scaled, train_target)
scores = cross_validate(kn, train_input, train_target, return_train_score=True, n_jobs=-1)
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

## 가우시안(정규분포)나이브 베이즈

**from sklearn.naive_bayes import GaussianNB**
classifier = GaussianNB()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 에이다 부스트

**from sklearn.ensemble import AdaBoostClassifier**
classifier = AdaBoostClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 이차판별분석

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
classifier = QuadraticDiscriminantAnalysis()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 서포트 벡터 머신

from sklearn.svm import SVC
classifier = SVC(kernel='linear')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 서포트 벡터 머신 - RBF 커널

from sklearn.svm import SVC
classifier = SVC(kernel='rbf')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 보팅

from sklearn.ensemble import VotingClassifier
clf1 = AdaBoostClassifier()
clf2 = RandomForestClassifier()
clf3 = SVC(kernel='linear')
classifier = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)])
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))

## 배깅

from sklearn.ensemble import  BaggingClassifier
classifier = BaggingClassifier(base_estimator=SVC(kernel='rbf'), n_estimators=10)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
skf = StratifiedKFold(n_splits=10, shuffle=True)
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f}%".format(accuracies.std()*100))