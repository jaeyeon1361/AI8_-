{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beeea3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd8e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('../pubg/456(수정).m4v')\n",
    "\n",
    "\n",
    "if cap is None : \n",
    "    print('Video read failed')\n",
    "    sys.exit()\n",
    "                    \n",
    "    \n",
    "while True : \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == False :\n",
    "        break\n",
    "    \n",
    "    if cv2.waitKey(10) == 27 :\n",
    "        break\n",
    "        \n",
    "    resize_frame = cv2.resize(frame, (1280, 720), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow('video', resize_frame)   \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feeabd4",
   "metadata": {},
   "source": [
    " BackgroundSubtractorMOG2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecec8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 잘되는데 주석처리 2208300927\n",
    "## 후보1, 백그라운드 마스크 처리를 통해서 제외된 것을 인식함\n",
    "# cv2.createBackgroundSubtractorMOG2() 함수 이용\n",
    "\n",
    "cap = cv2.VideoCapture('../pubg/final test 2.mp4')\n",
    "\n",
    "# 옵션 설명 http://layer0.authentise.com/segment-background-using-computer-vision.html\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=100, detectShadows=0)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    width = frame.shape[1]\n",
    "    height = frame.shape[0]\n",
    "    frame = cv2.resize(frame, (int(width/3), int(height/3)))\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(fgmask)\n",
    "\n",
    "\n",
    "    for index, centroid in enumerate(centroids): # 중심에 대한 순서값 부여\n",
    "        if stats[index][0] == 0 and stats[index][1] == 0:\n",
    "            continue\n",
    "        if np.any(np.isnan(centroid)):\n",
    "            # isnan 함수는 데이터가 nan(not a number)인지 아닌지를 판별해주는 함수입니다. \n",
    "            # nan이면 True를, nan이 아니면 False를 반환\n",
    "            continue\n",
    "\n",
    "\n",
    "        x, y, width, height, area = stats[index]\n",
    "        centerX, centerY = int(centroid[0]), int(centroid[1])\n",
    "\n",
    "        if area > 5:\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 0, 255))\n",
    "\n",
    "\n",
    "    cv2.imshow('mask',fgmask)\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c39c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## googlenet 으로 진행했던 것 응용, fullbody 인식\n",
    "## 후보2\n",
    "## 처리할게 많아서 그런지 너무느림, 구글링 결과 영상이 빨라질려고 하면, tensorflow를 통해서 gpu를 가동해야한다고함.. \n",
    "## 여러가지 시도를 해보았으나, 버전이 계속 맞지 않아서 그런지 기동불가\n",
    "\n",
    "cap = cv2.VideoCapture('../pubg/456(수정).m4v')\n",
    "\n",
    "body_cascade = cv2.CascadeClassifier('../googlenet/haarcascade_fullbody.xml')\n",
    "\n",
    "prev_time = 0\n",
    "FPS = 10\n",
    "\n",
    "if cap is None : \n",
    "    print('Video read failed')\n",
    "    sys.exit()\n",
    "                    \n",
    "while True : \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    grayframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    body = body_cascade.detectMultiScale(grayframe, 1.01, 10, minSize=(30, 30))\n",
    "    for (x,y,w,h) in body :         \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),3, 4, 0)\n",
    "    \n",
    "    #cv2.imshow('body', frame)\n",
    "    \n",
    "    if ret == False :\n",
    "        break\n",
    "    \n",
    "    if cv2.waitKey(10) == 27 :\n",
    "        break\n",
    "        \n",
    "    resize_frame = cv2.resize(frame, (960, 640), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow('video', resize_frame)   \n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21b43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read video file\n"
     ]
    }
   ],
   "source": [
    "## 2208260923 test 1\n",
    "## 후보3 (이게 가장 베스트)\n",
    "## 객체 드래그 해서 추적함\n",
    "## https://bkshin.tistory.com/entry/OpenCV-32-객체-추적을-위한-Tracking-API\n",
    "## https://rosia.tistory.com/243 -> opencv tracker 종류\n",
    "# 트랙커 객체 생성자 함수 리스트 ---①\n",
    "\n",
    "trackers = [cv2.TrackerMIL_create,\n",
    "            cv2.TrackerKCF_create,\n",
    "            cv2.TrackerGOTURN_create, # opencv tracking API 모듈들\n",
    "            cv2.TrackerCSRT_create]  # 모듈의 버젼이 다를 수 있으니, 모듈 확인 후 진행할 것 !!\n",
    "\n",
    "trackerIdx = 0  # 트랙커 생성자 함수 선택 인덱스\n",
    "tracker = None\n",
    "isFirst = True\n",
    "\n",
    "video_src = 0 # 비디오 파일과 카메라 선택 ---②\n",
    "video_src = \"../pubg/final test 1.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_src)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "\n",
    "win_name = 'Tracking APIs'\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Cannot read video file')\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    if tracker is None: # 트랙커 생성 안된 경우\n",
    "        cv2.putText(img_draw, \"spacebar start or stop\", \\\n",
    "            (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    else:\n",
    "        ok, bbox = tracker.update(frame)   # 새로운 프레임에서 추적 위치 찾기 ---③\n",
    "        (x,y,w,h) = bbox\n",
    "        if ok: # 추적 성공\n",
    "            cv2.rectangle(img_draw, (int(x), int(y)), (int(x + w), int(y + h)), \\\n",
    "                          (0,255,0), 2, 1)\n",
    "        else : # 추적 실패시\n",
    "            cv2.putText(img_draw, \"tracking failed.\", (100,80), \\\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "            \n",
    "    trackerName = tracker.__class__.__name__\n",
    "\n",
    "\n",
    "    cv2.imshow(win_name, img_draw)\n",
    "    \n",
    "    key = cv2.waitKey(50) & 0xff\n",
    "    # 스페이스 바 또는 비디오 파일 최초 실행 ---④\n",
    "    if key == ord(' ') or (video_src != 0 and isFirst): \n",
    "        isFirst = False\n",
    "        roi = cv2.selectROI(win_name, frame, False)  # 초기 객체 위치 설정\n",
    "        if roi[2] and roi[3]:         # 위치 설정 값 있는 경우\n",
    "            tracker = trackers[trackerIdx]()    #트랙커 객체 생성 ---⑤\n",
    "            isInit = tracker.init(frame, roi)\n",
    "    elif key in range(48, 56): # 0~7 숫자 입력   ---⑥\n",
    "        trackerIdx = key-48     # 선택한 숫자로 트랙커 인덱스 수정\n",
    "        if bbox is not None:\n",
    "            tracker = trackers[trackerIdx]() # 선택한 숫자의 트랙커 객체 생성 ---⑦\n",
    "            isInit = tracker.init(frame, bbox) # 이전 추적 위치로 추적 위치 초기화\n",
    "    elif key == 27 : \n",
    "        break\n",
    "else:\n",
    "    print( \"read is failed vedio\")\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb327535",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://deep-learning-study.tistory.com/275\n",
    "# 후보4\n",
    "## 실행은 되긴하나 HSV에 관한게 필요할 듯, 해당 알고리즘에 대한 이해 필요\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('../pubg/final test 3.mp4') # 비디오 파일 열기\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Video open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "x, y, w, h = 135, 220, 100, 100 # 초기 사각형 영역 설정: (x, y, w, h)\n",
    "rc = (x, y, w, h)\n",
    "\n",
    "ret, frame = cap.read() # 영상의 정보 받아오기\n",
    "\n",
    "if not ret:\n",
    "    print('frame read failed!')\n",
    "    sys.exit()\n",
    "    \n",
    "roi = frame[y:y+h, x:x+w]\n",
    "roi_hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "channels = [0,1] # # HS 히스토그램 계산, H와 S만 이용. V는 안씀\n",
    "ranges = [0, 180, 0, 256]\n",
    "# calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) -> hist\n",
    "hist = cv2.calcHist([roi_hsv], channels, None, [90, 128], ranges)\n",
    "\n",
    "# Mean Shift 알고리즘 종료 기준\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "# 비디오 매 프레임 처리\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # HS 히스토그램에 대한 역투영\n",
    "    # frame을 HSV로 변환\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # 히스토그램 역투영 확률 데이터 얻기\n",
    "    backproj = cv2.calcBackProject([frame_hsv], channels, hist, ranges, 1)\n",
    "    \n",
    "    # Mean Shift\n",
    "    # 역투영 확률값을 Mean shift 인자에 입력\n",
    "    _, rc = cv2.meanShift(backproj, rc, term_crit)\n",
    "    \n",
    "    # 추적 결과 화면 출력\n",
    "    cv2.rectangle(frame, rc, (0, 0, 255), 2)\n",
    "    resize_frame = cv2.resize(frame, (1280, 720), interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow('frame', resize_frame)\n",
    "    \n",
    "    if cv2.waitKey(60) == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcc1937",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: ../darknet-master/yolov3.cfg in function 'cv::dnn::dnn4_v20220524::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLO test\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#-- yolo 포맷 및 클래스명 불러오기\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../darknet-master/yolov3.cfg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../darknet-master/yolov3.weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#-- GPU 사용\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#-- 클래스(names파일) 오픈 / 본인 개발 환경에 맞게 변경할 것\u001b[39;00m\n\u001b[0;32m     67\u001b[0m classes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: ../darknet-master/yolov3.cfg in function 'cv::dnn::dnn4_v20220524::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "## 2208261529 되긴함 근데 프레임 오지게 끊김\n",
    "# 후보5\n",
    "# yolo 모델을 이용한 객체 추적, 진행하였으나, 후보 2처럼 tensorflow의 gps 를 연동하지 않으면 프레임 끊김은 지속될 것으로 확인\n",
    "# 나중에 좀 더 자세히 알 필요가 있음\n",
    "\n",
    "vedio_path = '../pubg/final test 3.mp4' #-- 사용할 영상 경로\n",
    "min_confidence = 0.5\n",
    "\n",
    "def detectAndDisplay(frame):\n",
    "    start_time = time.time()\n",
    "    img = cv2.resize(frame, None, fx=0.8, fy=0.8)\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    #-- 창 크기 설정\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    #-- 탐지한 객체의 클래스 예측 \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > min_confidence:\n",
    "                # 탐지한 객체 박싱\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "               \n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = \"{}: {:.2f}\".format(classes[class_ids[i]], confidences[i]*100)\n",
    "           # print(i, label)\n",
    "            color = colors[i] #-- 경계 상자 컬러 설정 / 단일 생상 사용시 (255,255,255)사용(B,G,R)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(img, label, (x, y - 5), font, 1, color, 1) \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    #  print(\"=== A frame took {:.3f} seconds\".format(process_time))\n",
    "    cv2.imshow(\"YOLO test\", img)\n",
    "    \n",
    "#-- yolo 포맷 및 클래스명 불러오기\n",
    "net = cv2.dnn.readNet('../darknet-master/yolov3.cfg','../darknet-master/yolov3.weights')\n",
    "\n",
    "#-- GPU 사용\n",
    "#net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "#-- 클래스(names파일) 오픈 / 본인 개발 환경에 맞게 변경할 것\n",
    "classes = []\n",
    "with open(\"../darknet-master/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "#-- 비디오 활성화\n",
    "cap = cv2.VideoCapture(vedio_path) #-- 웹캠 사용시 vedio_path를 0 으로 변경\n",
    "if not cap.isOpened:\n",
    "    # print('--(!)Error opening video capture')\n",
    "    exit(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        # print('--(!) No captured frame -- Break!')\n",
    "        break\n",
    "    detectAndDisplay(frame)\n",
    "    #-- q 입력시 종료\n",
    "    if cv2.waitKey(50) & 0xFF == 27 :\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266de97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
