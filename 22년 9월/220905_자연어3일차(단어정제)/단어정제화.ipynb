{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec15251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'you', 'ever', 'fallen', 'head', '.', 'over', 'heels', 'for', 'somebody', '.', 'Not', 'just', 'somebody', '.', 'No', 'no', '.', 'Rex', 'you', 'did', 'it', 'again', '.', 'Have', 'you', 'ever', 'fallen', 'head', '.', 'over', 'heels', 'for', 'somebody', '.', 'That', 'made', 'promises', '.', 'to', 'give', 'you', 'the', 'world', 'Um', '.', 'I', 'really', 'hope', 'they', 'held', 'you', 'down', '.', 'I', 'really', 'hope', 'it', 'was', 'no', 'lying', '.', 'Cause', 'when', 'heart', 'breaks', 'it', '.', 'feel', 'like', 'the', 'world', \"'s\", 'gone', '.', 'But', 'if', 'the', 'love', \"'s\", 'real', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, TreebankWordTokenizer, RegexpTokenizer, sent_tokenize\n",
    "\n",
    "text = \"\"\"\n",
    "Have you ever fallen head.\n",
    "over heels for somebody.\n",
    "Not just somebody.\n",
    "No no.\n",
    "Rex you did it again.\n",
    "Have you ever fallen head.\n",
    "over heels for somebody.\n",
    "That made promises.\n",
    "to give you the world Um.\n",
    "I really hope they held you down.\n",
    "I really hope it was no lying.\n",
    "Cause when heart breaks it.\n",
    "feel like the world's gone.\n",
    "But if the love's real.\n",
    "\"\"\"\n",
    "\n",
    "print(word_tokenize(text)) # 구두점(') 토큰화 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909a71fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'you', 'ever', 'fallen', 'head', '.', 'over', 'heels', 'for', 'somebody', '.', 'Not', 'just', 'somebody', '.', 'No', 'no', '.', 'Rex', 'you', 'did', 'it', 'again', '.', 'Have', 'you', 'ever', 'fallen', 'head', '.', 'over', 'heels', 'for', 'somebody', '.', 'That', 'made', 'promises', '.', 'to', 'give', 'you', 'the', 'world', 'Um', '.', 'I', 'really', 'hope', 'they', 'held', 'you', 'down', '.', 'I', 'really', 'hope', 'it', 'was', 'no', 'lying', '.', 'Cause', 'when', 'heart', 'breaks', 'it', '.', 'feel', 'like', 'the', 'world', \"'\", 's', 'gone', '.', 'But', 'if', 'the', 'love', \"'\", 's', 'real', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(text)) # 구두점(') 토큰화 O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61075896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'you', 'ever', 'fallen', 'head.', 'over', 'heels', 'for', 'somebody.', 'Not', 'just', 'somebody.', 'No', 'no.', 'Rex', 'you', 'did', 'it', 'again.', 'Have', 'you', 'ever', 'fallen', 'head.', 'over', 'heels', 'for', 'somebody.', 'That', 'made', 'promises.', 'to', 'give', 'you', 'the', 'world', 'Um.', 'I', 'really', 'hope', 'they', 'held', 'you', 'down.', 'I', 'really', 'hope', 'it', 'was', 'no', 'lying.', 'Cause', 'when', 'heart', 'breaks', 'it.', 'feel', 'like', 'the', 'world', \"'s\", 'gone.', 'But', 'if', 'the', 'love', \"'s\", 'real', '.']\n"
     ]
    }
   ],
   "source": [
    "print(TreebankWordTokenizer().tokenize(text)) # 표준화된 토큰화모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50402a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'you', 'ever', 'fallen', 'head', 'over', 'heels', 'for', 'somebody', 'Not', 'just', 'somebody', 'No', 'no', 'Rex', 'you', 'did', 'it', 'again', 'Have', 'you', 'ever', 'fallen', 'head', 'over', 'heels', 'for', 'somebody', 'That', 'made', 'promises', 'to', 'give', 'you', 'the', 'world', 'Um', 'I', 'really', 'hope', 'they', 'held', 'you', 'down', 'I', 'really', 'hope', 'it', 'was', 'no', 'lying', 'Cause', 'when', 'heart', 'breaks', 'it', 'feel', 'like', 'the', 'world', 's', 'gone', 'But', 'if', 'the', 'love', 's', 'real']\n"
     ]
    }
   ],
   "source": [
    "print(RegexpTokenizer('\\w+').tokenize(text)) # 정규표현식 활용 토크나이즈 ->뒤에서 자세히 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ef08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nHave you ever fallen head.', 'over heels for somebody.', 'Not just somebody.', 'No no.', 'Rex you did it again.', 'Have you ever fallen head.', 'over heels for somebody.', 'That made promises.', 'to give you the world Um.', 'I really hope they held you down.', 'I really hope it was no lying.', 'Cause when heart breaks it.', \"feel like the world's gone.\", \"But if the love's real.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text)) # 문장별 토크나이즈 ->바로 뒤에 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3423aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5\n"
     ]
    }
   ],
   "source": [
    "# ->int로 변형해서 보내줌 \n",
    "def funcName(x:str, y:float = 6.5)->int :\n",
    "    return x+y\n",
    "value = funcName(3)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94cfcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac2df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960b3f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize를 사용한 경우는 아래와 같습니다.\n",
      "['Love', 'looks', 'not', 'with', 'the', 'eyes', ',', 'but', 'with', 'the', 'mind', '.', 'And', 'therefore', 'is', 'wing', \"'d\", 'Cupid', 'painted', 'blind', '.']\n",
      "['South', 'Korea', 'population', 'is', '48,750,000']\n",
      "wordpunct_tokenize를 사용한 경우는 아래와 같습니다.\n",
      "['Love', 'looks', 'not', 'with', 'the', 'eyes', ',', 'but', 'with', 'the', 'mind', '.', 'And', 'therefore', 'is', 'wing', \"'\", 'd', 'Cupid', 'painted', 'blind', '.']\n",
      "['South', 'Korea', 'population', 'is', '48', ',', '750', ',', '000']\n",
      "Treebanktokenize를 사용한 경우는 아래와 같습니다.\n",
      "['Love', 'looks', 'not', 'with', 'the', 'eyes', ',', 'but', 'with', 'the', 'mind.', 'And', 'therefore', 'is', 'wing', \"'d\", 'Cupid', 'painted', 'blind', '.']\n",
      "['South', 'Korea', 'population', 'is', '48,750,000']\n"
     ]
    }
   ],
   "source": [
    "tb_tokenizer=TreebankWordTokenizer()\n",
    "\n",
    "text1 = \"Love looks not with the eyes, but with the mind. And therefore is wing'd Cupid painted blind.\"\n",
    "text2 = \"South Korea population is 48,750,000\"\n",
    "\n",
    "word_tok = word_tokenize(text1)\n",
    "word_tok2 = word_tokenize(text2)\n",
    "\n",
    "wordpunct_tok = WordPunctTokenizer().tokenize(text1)\n",
    "wordpunct_tok2 = WordPunctTokenizer().tokenize(text2)\n",
    "\n",
    "tb_tok = tb_tokenizer.tokenize(text1)\n",
    "tb_tok2 = tb_tokenizer.tokenize(text2)\n",
    "\n",
    "print(\"word_tokenize를 사용한 경우는 아래와 같습니다.\")\n",
    "print(word_tok)\n",
    "print(word_tok2)\n",
    "print(\"wordpunct_tokenize를 사용한 경우는 아래와 같습니다.\")\n",
    "print(wordpunct_tok)\n",
    "print(wordpunct_tok2)\n",
    "print(\"Treebanktokenize를 사용한 경우는 아래와 같습니다.\")\n",
    "print(tb_tok)\n",
    "print(tb_tok2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409197ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams #n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
    "                         # n=1일 때는 unigramm 2일 대는 bigram, 3일때는 trigram\n",
    "sentence = 'There is no royal road to learning'\n",
    "bigram = list(ngrams(sentence.split(), 2))\n",
    "print(bigram)\n",
    "# 두 단어를 보고 다음 글이 뭐가 나올지 예측해봐? 할때 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574d6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is', 'no', 'royal', 'road', 'to'), ('is', 'no', 'royal', 'road', 'to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "trigram = list(ngrams(sentence.split(), 3)) # 해보니까 계속 늘어남\n",
    "print(trigram)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4fe5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  # 불용어(Stop word)는 분석에 큰 의미가 없는 단어를 지칭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3ad79",
   "metadata": {},
   "source": [
    "the, a, an, is, I, my 등과 같이 문장을 구성하는 필수 요소지만 문맥적으로 큰 의미가 없는 단어가 이에 속함. 이런 불용어는 텍스트에 빈번하게 나타나기 때문에 중요한 단어로 인지될 수 있습니다. 하지만 실질적으로는 중요한 단어가 아니므로 사전에 제거해줘야 합니다.만약 big 같은 것들은 따로 불용어에서 제외 리스트를 만들어줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c0ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for token in word_tokens: \n",
    "    if token not in stop_words: \n",
    "        result.append(token) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe1471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Natural', 'Language', 'Toolkit', ',', 'or', 'more', 'commonly', 'NLTK', ',', 'is', 'a', 'suite', 'of', 'libraries', 'and', 'programs', 'for', 'symbolic', 'and', 'statistical', 'natural', 'language', 'processing', 'for', 'English', 'written', 'in', 'the', 'Python', 'programming'] \n",
      "\n",
      "['The', 'Natural', 'Language', 'Toolkit', ',', 'commonly', 'NLTK', ',', 'suite', 'libraries', 'programs', 'symbolic', 'statistical', 'natural', 'language', 'processing', 'English', 'written', 'Python', 'programming']\n"
     ]
    }
   ],
   "source": [
    "example = \"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens,'\\n') # 불용어를 제외 하기 전 text\n",
    "print(result)  # 불용어를 제외해서 is not and과 같은게 제외됌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97706bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화한 문장은['불용어란', '자주', '등장하지만', '데이터를', '분석하는데', '있어', '큰', '의미를', '갖지', '않는', '단어들을', '뜻합니다', '.', '불용어는', '임의로', '설정할', '수', '도', '있고', ',', '영문의', '불용어', '리스트의', '경우', 'NTLK', '라이브러리에서', '정의한', '불용어', '리스트를', '사용할', '수', '있습니다', '.', '다만', '한국어의', '경우', '조사와', '접속사의', '사용이', '다양하며', ',', '언어의', '변형이', '많기', '때문에', '직접', '정의하는게', '좋습니다', '.']입니다. \n",
      "\n",
      "불용어를 제거하면['불용어란', '등장하지만', '데이터를', '분석하는데', '있어', '큰', '의미를', '갖지', '않는', '단어들을', '뜻합니다', '.', '불용어는', '임의로', '설정할', '수', '도', '있고', ',', '영문의', '불용어', '리스트의', '경우', 'NTLK', '라이브러리에서', '정의한', '불용어', '리스트를', '사용할', '수', '있습니다', '.', '다만', '한국어의', '경우', '조사와', '접속사의', '사용이', '다양하며', ',', '언어의', '변형이', '많기', '때문에', '직접', '정의하는게', '좋습니다', '.']입니다\n"
     ]
    }
   ],
   "source": [
    "word = \"불용어란 자주 등장하지만 데이터를 분석하는데 있어 큰 의미를 갖지 않는 단어들을 뜻합니다. 불용어는 임의로 설정할 수 도 있고, 영문의 불용어 리스트의 경우 NTLK 라이브러리에서 정의한 불용어 리스트를 사용할 수 있습니다. 다만 한국어의 경우 조사와 접속사의 사용이 다양하며, 언어의 변형이 많기 때문에 직접 정의하는게 좋습니다.\"\n",
    "\n",
    "stop =\"자주,종종,가끔,많이\"\n",
    "\n",
    "#예시로 설정한 불용어를 ,을 기준으로 잘라냅니다.\n",
    "stop_list = stop.split(',')\n",
    "tok = word_tokenize(word)\n",
    "\n",
    "def stopword(word_tokenize):\n",
    "    result = []\n",
    "\n",
    "    for w in word_tokenize:\n",
    "        #임의로 정의한 불용어가 아닌 경우만 result에 추가합니다.\n",
    "        if w not in stop_list:\n",
    "            result.append(w)\n",
    "\n",
    "    return result\n",
    "\n",
    "print('토큰화한 문장은'+ str(tok) + '입니다.', '\\n')\n",
    "print('불용어를 제거하면' + str(stopword(tok)) + '입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345903f1",
   "metadata": {},
   "source": [
    "한국어의 토큰화는 애매한 경우가 많기 때문에 다하고 나서 확인이 꼭꼭 필요하다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a07a0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c6ebb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7489daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stem(tokenize_list):\n",
    "    p = PorterStemmer()\n",
    "    result = []\n",
    "\n",
    "    for w in tokenize_list:\n",
    "        result.append(p.stem(w))\n",
    "\n",
    "    return result\n",
    "\n",
    "def lancaster_stem(tokenize_list):\n",
    "    l = LancasterStemmer()\n",
    "    result = []\n",
    "\n",
    "    for w in tokenize_list:\n",
    "        result.append(l.stem(w))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac5565c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(origin, compare):\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(origin)):\n",
    "        if str(origin[i]) == str(compare[i]):\n",
    "            pass\n",
    "        else:\n",
    "            result.append(compare[i])\n",
    "\n",
    "    print('\\033[1m' + str(result) + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e1a8a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize를 사용한 경우는 아래와 같습니다.\n",
      "['The', 'necrosis', 'is', 'closely', 'related', 'to', 'the', 'flow', 'of', 'blood', '.', 'So', 'we', 'can', 'make', 'a', 'diagnosis', 'of', 'tissue', 'necrosis', 'to', 'observe', 'the', 'flow', 'of', 'blood', '.']\n",
      "아래는 포터알고리즘 어간추출(stemming)의 결과입니다.\n",
      "['the', 'necrosi', 'is', 'close', 'relat', 'to', 'the', 'flow', 'of', 'blood', '.', 'so', 'we', 'can', 'make', 'a', 'diagnosi', 'of', 'tissu', 'necrosi', 'to', 'observ', 'the', 'flow', 'of', 'blood', '.']\n",
      "아래는 란체스터알고리즘 어간추출(stemming)의 결과입니다.\n",
      "['the', 'necros', 'is', 'clos', 'rel', 'to', 'the', 'flow', 'of', 'blood', '.', 'so', 'we', 'can', 'mak', 'a', 'diagnos', 'of', 'tissu', 'necros', 'to', 'observ', 'the', 'flow', 'of', 'blood', '.']\n",
      "아래는 porter 알고리즘과 lancaster 알고리즘 결과의 차이입니다.\n",
      "\u001b[1m['the', 'necrosi', 'close', 'relat', 'so', 'diagnosi', 'tissu', 'necrosi', 'observ']\u001b[0m\n",
      "\u001b[1m['the', 'necros', 'clos', 'rel', 'so', 'mak', 'diagnos', 'tissu', 'necros', 'observ']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text = \"The necrosis is closely related to the flow of blood. So we can make a diagnosis of tissue necrosis to observe the flow of blood.\"\n",
    "#ref. S.H.LEE el. 2. \"Development of Laser Speckle Contrast Imaging System for early diagnosing tissue necrosis.The Korea Society of Medical & Biological Engineering 2019 Oct 055\"\n",
    "\n",
    "word_tok= word_tokenize(text)\n",
    "\n",
    "t1_p_stem = porter_stem(word_tok)\n",
    "\n",
    "t1_l_stem = lancaster_stem(word_tok)\n",
    "\n",
    "print(\"word_tokenize를 사용한 경우는 아래와 같습니다.\")\n",
    "print(word_tok)\n",
    "\n",
    "print(\"아래는 포터알고리즘 어간추출(stemming)의 결과입니다.\")\n",
    "print(t1_p_stem)\n",
    "\n",
    "print(\"아래는 란체스터알고리즘 어간추출(stemming)의 결과입니다.\")\n",
    "print(t1_l_stem)\n",
    "\n",
    "print(\"아래는 porter 알고리즘과 lancaster 알고리즘 결과의 차이입니다.\")\n",
    "compare(word_tok, t1_p_stem)\n",
    "compare(word_tok, t1_l_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4518da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookery\n",
      "inside\n"
     ]
    }
   ],
   "source": [
    "## 사용자가 지정한 정규표현을 기준으로 동작\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing')\n",
    "print(stemmer.stem('cooking'))\n",
    "print(stemmer.stem('cookery'))\n",
    "print(stemmer.stem('inginside'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2289629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hol\n"
     ]
    }
   ],
   "source": [
    "# 영어 외의 13개 국가의 언어에 대한 stemming을 지원\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "print(spanish_stemmer.stem('hola'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff7e7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7824305",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdbefbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "#정제와 단어 토큰화\n",
    "vocab = {} # 파이썬의 dictionary 자료형\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8eca655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ed87d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60a9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "#높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : #빈도수가 적은 단어는 제외한다.\n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca019f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a572b9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [8, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 8, 1, 8], [1, 8, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "#word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
    "\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf288513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db1aee17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1, 2, 3\n",
    "b = 4, 5, 6\n",
    "np.vstack([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab83a700",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "np.concatenate((a,b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e36dc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "749bdf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ae92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
