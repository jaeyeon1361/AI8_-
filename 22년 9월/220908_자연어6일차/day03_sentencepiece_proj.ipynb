{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNCR/Md/OJ4eNUm9PU5/ct5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 네이버 영화 리뷰 감성 분석"],"metadata":{"id":"0EMxryw2mhfs"}},{"cell_type":"markdown","source":["## 1. 데이터 준비와 확인"],"metadata":{"id":"2VNvRliFmjWs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtK_GmXumde4"},"outputs":[],"source":["import pandas as pd\n","import urllib.request\n","import matplotlib.pyplot as plt\n","import re\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from collections import Counter"]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\") # train\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\") # test\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\") # train + test"],"metadata":{"id":"BFqzXEF9mlOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_table('ratings_train.txt')\n","test_data = pd.read_table('ratings_test.txt')"],"metadata":{"id":"GF9YN8NrmmHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[:20]"],"metadata":{"id":"vkBhy5fcmnG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git \n","%cd Mecab-ko-for-Google-Colab/\n","!bash install_mecab-ko_on_colab190912.sh\n","%cd ../"],"metadata":{"id":"pTTZ9mthmoRE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. 데이터로더 구성"],"metadata":{"id":"faxKhXXmmrX0"}},{"cell_type":"code","source":["from konlpy.tag import Mecab\n","tokenizer = Mecab()"],"metadata":{"id":"Xj1GbOykmpj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"],"metadata":{"id":"HXzvSSNImtek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_remove_stopwords(data, stopwords, tokenizer):\n","    result = []\n","\n","    for sentence in data:\n","        curr_data = []\n","        curr_data = tokenizer.morphs(sentence) # mecab 형태소 분석 tokenizer\n","        curr_data = [word for word in curr_data if not word in stopwords] # 불용어 제거\n","        result.append(curr_data)\n","    return result"],"metadata":{"id":"kx1lnFjvmubk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(train_data, test_data, num_words=10000):\n","\n","    # 중복 제거\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","\n","    # Nan 결측치 제거\n","    train_data = train_data.dropna(how='any')\n","    test_data = test_data.dropna(how='any')\n","\n","    # 토큰화 및 불용어 제거\n","    x_train = tokenize_and_remove_stopwords(train_data['document'], stopwords, tokenizer)\n","    x_test = tokenize_and_remove_stopwords(test_data['document'], stopwords, tokenizer)\n","\n","    # 단어장 만드는 중...\n","    words = np.concatenate(x_train).tolist()\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4)\n","    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","\n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n","    \n","    x_train = list(map(wordlist_to_indexlist, x_train))\n","    x_test = list(map(wordlist_to_indexlist, x_test))\n","\n","    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index"],"metadata":{"id":"ya3C6KrKmvn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"],"metadata":{"id":"FQp7eQSTmwtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train[10])"],"metadata":{"id":"hX-CDRUfmx2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_word = {index:word for word, index in word_to_index.items()}"],"metadata":{"id":"Q7s9THDamy58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수\n","# 단, 모든 문장은 <BOS>로 시작하는 것을 말합니다.\n","\n","def get_encoded_sentence(sentence, word_to_index): ##### 텍스트 -> 숫자\n","    return [word_to_index['<BOS>']] + [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. ##### 숫자 -> 텍스트\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])\n","\n","# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"],"metadata":{"id":"vkNligi4mz0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_decoded_sentence(x_train[10], index_to_word)"],"metadata":{"id":"c9tlEYulm1bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. 모델 구성을 위한 데이터 분석 및 가공"],"metadata":{"id":"z3g6Pbvpm3mh"}},{"cell_type":"code","source":["# 데이터 셋 내 문장 길이 분포\n","total_data_text = list(x_train) + list(x_test)\n","\n","# 텍스트데이터 문장길이의 리스트를 생성한 후\n","num_tokens = [len(tokens) for tokens in total_data_text]\n","num_tokens = np.array(num_tokens)\n","\n","# 문장 길이의 평균값, 최대값, 표준편차를 계산\n","print('문장길이 평균 :', np.mean(num_tokens))\n","print('문장길이 최대 :', np.max(num_tokens))\n","print('문장길이 표준편차 : ', np.std(num_tokens))\n","\n","# 예를 들면 최대길이를 (평균 + 2*표준편차)로 한다면,\n","max_tokens = np.mean(num_tokens) +2 * np.std(num_tokens)\n","\n","maxlen = int(max_tokens)\n","print('pad_sequences maxlen : ', maxlen)\n","print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens)/len(num_tokens)))"],"metadata":{"id":"h7E9q1aam2g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 패딩 추가\n","x_train = pad_sequences(x_train, value=word_to_index['<PAD>'], padding='pre', maxlen=maxlen)\n","x_test = pad_sequences(x_test, value=word_to_index['<PAD>'], padding='pre', maxlen=maxlen)"],"metadata":{"id":"uBjB4wBvm6IF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train.shape)\n","print(x_test.shape)"],"metadata":{"id":"rx6fTmEam841"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. 모델 구성 및 validation 구성"],"metadata":{"id":"nlgB4OHtm-Tl"}},{"cell_type":"code","source":["vocab_size = 10000\n","word_vector_dim = 200 # 2의 배수\n","\n","model = keras.Sequential()\n","model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(keras.layers.LSTM(8))\n","model.add(keras.layers.Dense(8, activation='relu'))\n","model.add(keras.layers.Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"id":"XW4Vq7xCm-FF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 훈련"],"metadata":{"id":"vJYF4w8tnA9d"}},{"cell_type":"code","source":["x_val = x_train[:50000]\n","y_val = y_train[:50000]\n","\n","partial_x_train = x_train[50000:]\n","partial_y_train = y_train[50000:]"],"metadata":{"id":"NLxDwKS3nBVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","epochs= 100\n","history = model.fit(partial_x_train, partial_y_train, epochs=epochs, batch_size=512, validation_data=(x_val, y_val), verbose=1)"],"metadata":{"id":"7QlaFTBOnC69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = model.evaluate(x_test, y_test, verbose=2)"],"metadata":{"id":"vRHr6cC7nE8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(results)"],"metadata":{"id":"OHpiZY_PnGM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. loss, accuracy 그래프 시각화"],"metadata":{"id":"ZB_YwdA2nHil"}},{"cell_type":"code","source":["history_dict = history.history\n","print(history_dict.keys())"],"metadata":{"id":"RsBgi2-HnH5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history_dict['accuracy']\n","val_acc = history_dict['val_accuracy']\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']"],"metadata":{"id":"pH7gc6M2nJM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = range(1, len(acc)+1)\n","\n","plt.plot(epochs, loss, 'r-', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"1ZaR-vVvnKyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.clf() #그림을 초기화\n","\n","plt.plot(epochs, acc, 'r-', label= 'Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"o0jd9ll5nL6F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Proj: 네이버 영화 리뷰 감성 분류에 SentencePiece적용하기"],"metadata":{"id":"ALn5PrnBnNfl"}},{"cell_type":"markdown","source":["- 네이버 영화리뷰 감정 분석 코퍼스에 SentencePiece를 적용시킨 모델 학습하기\n","- 학습된 모델로 sp_tokenize() 메소드 구현하기\n","- 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정 분석 모델을 재학습하기\n","- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기\n","- (보너스) SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기"],"metadata":{"id":"8iY5TwvgnO8F"}},{"cell_type":"code","source":["!wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz"],"metadata":{"id":"j91iS9QGnN4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tar -xzvf korean-english-park.train.tar.gz"],"metadata":{"id":"I8nrCsxVnQ21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```\n","vocab_size = 8000\n","spm.SentencePieceTrainer.Train(\n","    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size) --model_type==bpe\n",")\n","```"],"metadata":{"id":"nehjes1YnS5F"}},{"cell_type":"markdown","source":["```\n","s = spm.SentencePieceProcessor()\n","s.Load('korean_spm.model')\n","\n","def sp_tokenize(s, corpus):\n","\n","    tensor = []\n","\n","    for sen in corpus:\n","        tensor.append(s.EncodeAsIds(sen))\n","\n","    with open(\"./korean_spm.vocab\", 'r') as f:\n","        vocab = f.readlines()\n","\n","    word_index = {}\n","    index_word = {}\n","\n","    for idx, line in enumerate(vocab):\n","        word = line.split(\"\\t\")[0]\n","\n","        word_index.update({idx:word})\n","        index_word.update({word:idx})\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n","\n","    return tensor, word_index, index_word\n","```"],"metadata":{"id":"Ryc0m-tJnUY9"}},{"cell_type":"markdown","source":["```\n","1) 매개변수로 토큰화된 문장의 list를 전달하는 대신 온전한 문장의 list 를 전달합니다.\n","\n","2) 생성된 vocab 파일을 읽어와 { <word> : <idx> } 형태를 가지는 word_index 사전과 { <idx> : <word>} 형태를 가지는 index_word 사전을 생성하고 함께 반환합니다.\n","\n","3) 리턴값인 tensor 는 앞의 함수와 동일하게 토큰화한 후 Encoding된 문장입니다. 바로 학습에 사용할 수 있게 Padding은 당연히 해야겠죠?\n","```"],"metadata":{"id":"uV85iyXAncUM"}},{"cell_type":"markdown","source":["```\n","#sp_tokenize(s, corpus) 사용예제\n","\n","my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n","tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n","print(tensor)\n","```"],"metadata":{"id":"5pJlBxpynekk"}},{"cell_type":"code","source":[],"metadata":{"id":"TpcdiKutnTLk"},"execution_count":null,"outputs":[]}]}